apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2023-03-08T11:42:24Z"
  generateName: machine-approver-9579cdcb5-
  labels:
    app: machine-approver
    pod-template-hash: 9579cdcb5
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:target.workload.openshift.io/management: {}
        f:generateName: {}
        f:labels:
          .: {}
          f:app: {}
          f:pod-template-hash: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"a9bb65e4-02f9-41b8-a4b0-cc03cd106658"}: {}
      f:spec:
        f:containers:
          k:{"name":"kube-rbac-proxy"}:
            .: {}
            f:args: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:ports:
              .: {}
              k:{"containerPort":9192,"protocol":"TCP"}:
                .: {}
                f:containerPort: {}
                f:hostPort: {}
                f:name: {}
                f:protocol: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/etc/kube-rbac-proxy"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/etc/tls/private"}:
                .: {}
                f:mountPath: {}
                f:name: {}
          k:{"name":"machine-approver-controller"}:
            .: {}
            f:args: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"METRICS_PORT"}:
                .: {}
                f:name: {}
                f:value: {}
              k:{"name":"RELEASE_VERSION"}:
                .: {}
                f:name: {}
                f:value: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/var/run/configmaps/config"}:
                .: {}
                f:mountPath: {}
                f:name: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:hostNetwork: {}
        f:nodeSelector: {}
        f:priorityClassName: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:serviceAccount: {}
        f:serviceAccountName: {}
        f:terminationGracePeriodSeconds: {}
        f:tolerations: {}
        f:volumes:
          .: {}
          k:{"name":"auth-proxy-config"}:
            .: {}
            f:configMap:
              .: {}
              f:defaultMode: {}
              f:name: {}
            f:name: {}
          k:{"name":"config"}:
            .: {}
            f:configMap:
              .: {}
              f:defaultMode: {}
              f:name: {}
              f:optional: {}
            f:name: {}
          k:{"name":"machine-approver-tls"}:
            .: {}
            f:name: {}
            f:secret:
              .: {}
              f:defaultMode: {}
              f:secretName: {}
    manager: kube-controller-manager
    operation: Update
    time: "2023-03-08T11:42:24Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          .: {}
          k:{"type":"PodScheduled"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:message: {}
            f:reason: {}
            f:status: {}
            f:type: {}
    manager: kube-scheduler
    operation: Update
    subresource: status
    time: "2023-03-08T11:42:25Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.0.0.3"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    subresource: status
    time: "2023-03-08T11:55:19Z"
  name: machine-approver-9579cdcb5-hjb8k
  namespace: openshift-cluster-machine-approver
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: machine-approver-9579cdcb5
    uid: a9bb65e4-02f9-41b8-a4b0-cc03cd106658
  resourceVersion: "22509"
  uid: 66ad6c13-d432-4c37-90ca-3b03fdc0b814
spec:
  containers:
  - args:
    - --secure-listen-address=0.0.0.0:9192
    - --upstream=http://127.0.0.1:9191/
    - --tls-cert-file=/etc/tls/private/tls.crt
    - --tls-private-key-file=/etc/tls/private/tls.key
    - --config-file=/etc/kube-rbac-proxy/config-file.yaml
    - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
    - --logtostderr=true
    - --v=3
    image: registry.ci.openshift.org/origin/4.12-2023-03-04-063026@sha256:ff07afaff7fff38e3124162c5791ba969bcd750be02d5878f0387aed62517fa9
    imagePullPolicy: IfNotPresent
    name: kube-rbac-proxy
    ports:
    - containerPort: 9192
      hostPort: 9192
      name: https
      protocol: TCP
    resources:
      requests:
        cpu: 10m
        memory: 20Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /etc/kube-rbac-proxy
      name: auth-proxy-config
    - mountPath: /etc/tls/private
      name: machine-approver-tls
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-khm9v
      readOnly: true
  - args:
    - --config=/var/run/configmaps/config/config.yaml
    - -v=2
    - --logtostderr
    - --leader-elect=true
    - --leader-elect-lease-duration=137s
    - --leader-elect-renew-deadline=107s
    - --leader-elect-retry-period=26s
    - --leader-elect-resource-namespace=openshift-cluster-machine-approver
    - --api-group-version=machine.openshift.io/v1beta1
    command:
    - /usr/bin/machine-approver
    env:
    - name: RELEASE_VERSION
      value: 4.12.0-0.okd-2023-03-04-063026
    - name: METRICS_PORT
      value: "9191"
    image: registry.ci.openshift.org/origin/4.12-2023-03-04-063026@sha256:7e5ab6b97326ed2a498974d89abcd5a1f777b5a9f99552ca3bc29d479da6b9f4
    imagePullPolicy: IfNotPresent
    name: machine-approver-controller
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /var/run/configmaps/config
      name: config
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-khm9v
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  hostNetwork: true
  nodeName: vrutkovs-mrmwt-master-2.c.openshift-gce-devel.internal
  nodeSelector:
    node-role.kubernetes.io/master: ""
  preemptionPolicy: PreemptLowerPriority
  priority: 2000000000
  priorityClassName: system-cluster-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: machine-approver-sa
  serviceAccountName: machine-approver-sa
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 120
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 120
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
  volumes:
  - configMap:
      defaultMode: 420
      name: kube-rbac-proxy
    name: auth-proxy-config
  - name: machine-approver-tls
    secret:
      defaultMode: 420
      secretName: machine-approver-tls
  - configMap:
      defaultMode: 440
      name: machine-approver-config
      optional: true
    name: config
  - name: kube-api-access-khm9v
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
      - configMap:
          items:
          - key: service-ca.crt
            path: service-ca.crt
          name: openshift-service-ca.crt
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T11:44:54Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T11:55:12Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T11:55:12Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T11:44:54Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://1c0832fa9d9ed28b5ba18489eb546941a23d0b8f71b4881e529a5a893e852c2f
    image: registry.ci.openshift.org/origin/4.12-2023-03-04-063026@sha256:ff07afaff7fff38e3124162c5791ba969bcd750be02d5878f0387aed62517fa9
    imageID: registry.ci.openshift.org/origin/4.12-2023-03-04-063026@sha256:ff07afaff7fff38e3124162c5791ba969bcd750be02d5878f0387aed62517fa9
    lastState: {}
    name: kube-rbac-proxy
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2023-03-08T11:45:59Z"
  - containerID: cri-o://36b8753315597b1226bcd8662f7f0f09ef5b5b244c0c7c20b7e3b98bf7395dc5
    image: registry.ci.openshift.org/origin/4.12-2023-03-04-063026@sha256:7e5ab6b97326ed2a498974d89abcd5a1f777b5a9f99552ca3bc29d479da6b9f4
    imageID: registry.ci.openshift.org/origin/4.12-2023-03-04-063026@sha256:7e5ab6b97326ed2a498974d89abcd5a1f777b5a9f99552ca3bc29d479da6b9f4
    lastState:
      terminated:
        containerID: cri-o://70c1522707443a74d00ca9f6c9334e20906cfb60941f587f7b134e798985b720
        exitCode: 255
        finishedAt: "2023-03-08T11:55:10Z"
        message: |
          308 11:52:14.058437       1 csr_check.go:182] Failed to retrieve current serving cert: remote error: tls: internal error
          I0308 11:52:14.058459       1 csr_check.go:202] Falling back to machine-api authorization for vrutkovs-mrmwt-worker-a-92xck.c.openshift-gce-devel.internal
          I0308 11:52:14.111011       1 controller.go:240] CSR csr-r686k approved
          I0308 11:52:14.149940       1 controller.go:121] Reconciling CSR: csr-gch6f
          I0308 11:52:14.229600       1 csr_check.go:157] csr-gch6f: CSR does not appear to be client csr
          I0308 11:52:14.256899       1 csr_check.go:545] retrieving serving cert from vrutkovs-mrmwt-worker-b-5rcfz.c.openshift-gce-devel.internal (10.0.128.3:10250)
          I0308 11:52:14.262102       1 csr_check.go:182] Failed to retrieve current serving cert: remote error: tls: internal error
          I0308 11:52:14.262125       1 csr_check.go:202] Falling back to machine-api authorization for vrutkovs-mrmwt-worker-b-5rcfz.c.openshift-gce-devel.internal
          I0308 11:52:14.279851       1 controller.go:240] CSR csr-gch6f approved
          E0308 11:54:23.401288       1 leaderelection.go:330] error retrieving resource lock openshift-cluster-machine-approver/cluster-machine-approver-leader: the server was unable to return a response in the time allotted, but may still be processing the request (get leases.coordination.k8s.io cluster-machine-approver-leader)
          E0308 11:55:10.399344       1 leaderelection.go:330] error retrieving resource lock openshift-cluster-machine-approver/cluster-machine-approver-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-cluster-machine-approver/leases/cluster-machine-approver-leader": context deadline exceeded
          I0308 11:55:10.399406       1 leaderelection.go:283] failed to renew lease openshift-cluster-machine-approver/cluster-machine-approver-leader: timed out waiting for the condition
          E0308 11:55:10.399438       1 leaderelection.go:306] Failed to release lock: resource name may not be empty
          F0308 11:55:10.399469       1 main.go:226] unable to run the manager: leader election lost
        reason: Error
        startedAt: "2023-03-08T11:46:22Z"
    name: machine-approver-controller
    ready: true
    restartCount: 1
    started: true
    state:
      running:
        startedAt: "2023-03-08T11:55:11Z"
  hostIP: 10.0.0.3
  phase: Running
  podIP: 10.0.0.3
  podIPs:
  - ip: 10.0.0.3
  qosClass: Burstable
  startTime: "2023-03-08T11:44:54Z"
